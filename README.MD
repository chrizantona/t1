# VibeCode — AI-платформа для техсобеседований на базе SciBox LLM

**VibeCode** — платформа для проведения технических собеседований с ИИ-интервьюером.  
Кандидат решает задачи в браузере, общается с ИИ, получает адаптивные задачи и честный отчёт по навыкам.  
Компания получает прозрачную оценку грейда, метрики и анти-чит.

Стек:
- **Frontend**: React + TypeScript
- **Backend**: FastAPI (Python)
- **DB**: PostgreSQL
- **Infra**: Docker + docker-compose
- **LLM**: SciBox LLM  
  - `qwen3-32b-awq` — универсальная чат-модель  
  - `qwen3-coder-30b-a3b-instruct-fp8` — код-ассистент  
  - `bge-m3` — эмбеддинги (опционально, для RAG / поиска)

---

## 1. Цели и соответствие ТЗ хакатона

### 1.1. Что делает платформа

Платформа должна:

1. Проводить **полностью автоматизированное техсобеседование** с ИИ-интервьюером:
   - Генерация адаптивных задач по выбранному направлению (backend / frontend / algorithms / …).
   - Диалог с кандидатом (вопросы про подход, сложность, граничные случаи).
   - Объяснения и ревью решений.
   - Финальный текстовый отчёт по результатам интервью.

2. Предоставлять **браузерную IDE**:
   - Подсветка синтаксиса, несколько языков.
   - Запуск кода в изолированных Docker-контейнерах.
   - Видимые и скрытые тесты.

3. Собирать **метрики**:
   - время решения;
   - количество попыток;
   - процент пройденных тестов;
   - устойчивость решения (robustness);
   - использование подсказок.

4. Реализовать **анти-чит**:
   - отслеживание копипаста;
   - подозрительных действий;
   - оценку похожести кода на LLM-генерацию.

5. Давать **интерфейс для компаний**:
   - итоговый грейд;
   - карта навыков;
   - trust score (доверие к результату);
   - экспорт отчётов.

---

## 2. Killer-фичи (умный ансамбль)

Ниже — фичи, которые делают VibeCode уникальным и дают весомый буст на демо.

### 2.1. CV-based Level Suggestion — авто-рекомендация уровня по резюме

**Идея:** кандидат может либо сам выбрать уровень, либо загрузить резюме, и система предложит **рекомендуемый грейд и направление**.

Flow:

1. Кандидат загружает резюме (PDF/текст).
2. Backend:
   - извлекает текст (простым парсером / библиотекой);
   - отправляет в SciBox LLM (`qwen3-32b-awq`) с промптом:
     - выделить опыт по годам;
     - стеки и ключевые технологии;
     - рекомендовать грейд (`junior / middle / middle+ / senior`);
     - предложить основное направление (backend, frontend, data и т.п.).
3. Frontend:
   - показывает карточку:
     - «Мы рекомендуем начать с уровня Middle по направлению Backend»;
     - краткое объяснение «почему» (отрывки из опыта).
   - Кандидат может:
     - согласиться;
     - выбрать другой уровень вручную.

Это выглядит как интеллектуальный **pre-screen** до интервью.

---

### 2.2. Grade Progress Bar + Skill Radar — прогресс по грейдам и карта навыков

**Идея:** вместо сухого «вы Middle» — показываем:

- где именно кандидат между грейдами;
- какие навыки нужно подтянуть до следующего уровня.

Компоненты:

1. **Грейд-прогресс-бар**  
   Шкала, например:
   `Junior → Junior+ → Middle → Middle+ → Senior`

   - мы считаем `overall_score` (0–100) из метрик;
   - маппим его на сегменты шкалы;
   - визуально показываем:
     - текущий грейд (например, Middle);
     - «прогресс до следующего» (например, до Middle+ не хватает 12 баллов).

2. **Skill Radar (карта навыков)**  
   Оси, например:
   - Алгоритмы и структуры данных
   - Архитектура и дизайн
   - Чистый код
   - Дебаг и тестирование
   - Коммуникация и объяснение

   Мы собираем сырые данные:
   - как быстро решает задачи;
   - сколько ошибок и перезапусков;
   - как объясняет решения в чате;
   - сколько подсказок использует.

   Дальше:
   - отправляем метрики + куски кода + части переписки в LLM;
   - модель возвращает JSON с оценкой по каждой оси (0–100) и коротким комментарием.

Пример JSON:

```json
{
  "overall_grade": "middle",
  "overall_score": 74,
  "skills": {
    "algorithms": { "score": 80, "comment": "Уверенно решает типовые задачи" },
    "architecture": { "score": 65, "comment": "Не всегда думает о масштабируемости" },
    "clean_code": { "score": 70, "comment": "Код читаемый, но можно улучшить нейминг" },
    "debugging": { "score": 60, "comment": "Медленно находит ошибки в краевых случаях" },
    "communication": { "score": 75, "comment": "Хорошо объясняет решения" }
  },
  "next_grade_tips": [
    "Больше практиковать задачи на оптимизацию по памяти",
    "Добавлять комментарии к сложным участкам кода",
    "Думать о масштабируемости перед написанием решения"
  ]
}
```

На фронте это красиво рисуется в виде радари и прогресс-бара.

---

### 2.3. Hint Economy — подсказки за цену

**Идея:** подсказки помогают кандидату не застревать, но **уменьшают максимально возможный балл** за задачу.

Уровни подсказок:

- Лёгкая подсказка — наводящий вопрос (−10% max score).
- Средняя подсказка — подсказка идеи решения (−20%).
- Жёсткая подсказка — почти псевдокод (−35%).

Flow:

1. В `TaskView` есть кнопка «Подсказки» с выбором уровня.
2. Backend (`POST /api/interview/hint`):
   - принимает `task_id`, `hint_level`, текущий код;
   - вызывает SciBox LLM с промптом уровня подсказки;
   - уменьшает `max_score` задачи и пишет запись в `hints`.
3. Frontend:
   - показывает подсказку;
   - обновляет отображение «максимальный возможный балл за эту задачу: 80/100».

В финальном отчёте:

- видно, сколько подсказок кандидат использовал;
- LLM комментирует, как это повлияло на общий грейд.

---

### 2.4. AI Bug Hunter — LLM как генератор новых тестов

**Идея:** после того, как кандидат прошёл базовые тесты, LLM превращается в **Bug Hunter**:

- анализирует код и задачу;
- придумывает сложные, граничные и «адверсариальные» входы.

Flow:

1. После успешного прохождения видимых тестов:
   - backend вызывает `qwen3-coder-30b-a3b-instruct-fp8`:
     - даём задачу, код, уже существующие тесты;
     - просим сгенерировать 3–5 входов, которые могут сломать решение.
2. Эти входы прогоняем через `code_runner`.
3. Если находятся ошибки:
   - помечаем задачу как «unstable»;
   - показываем кандидату блок «AI нашёл баг» с конкретным кейсом;
   - уменьшаем `robustness_score`.

Плюс: показывает, что у нас не просто статичные тесты, а **динамический ИИ-краш-тест** решений.

---

### 2.5. Explanation Check — проверка понимания, а не только кода

**Идея:** мы хотим оценивать не только то, что код работает, но и **понимает ли кандидат, что он делает**.

Flow:

1. После успешного решения задачи:
   - фронт показывает модалку:
     - «Опиши кратко, как работает твой алгоритм»;
     - «Какова асимптотическая сложность по времени и памяти?»
2. Backend:
   - отправляет задачу, код и объяснение в SciBox LLM;
   - просит вернуть JSON с:
     - `communication_score` (0–100),
     - `understanding_level` (низкий / средний / глубокий),
     - `comment`.
3. Добавляем эти данные в карты навыков и финальный отчёт.

Это усиливает ось `communication` и даёт плюс к «мозговитости» системы.

---

### 2.6. Trust Score + AI-Likeness — доверие к результату и похожесть на AI

**Trust Score**

- Собираем события на фронте:
  - copy/paste, devtools, фокус/расфокус окна, аномально быстрый ввод.
- Backend:
  - считает `trust_score` (0–100);
  - логирует все события в `anti_cheat_events`.

**AI-Likeness**

- Для финального решения отправляем код в `qwen3-coder-30b`:
  - просим оценить, насколько он похож на LLM-сгенерированный (0–100).
- Получаем:
  - `ai_likeness_score`;
  - комментарий, почему (стиль, паттерны, форматирование).

В отчёте для компании:

- видно:
  - грейд;
  - trust_score;
  - ai_likeness_score.
- LLM даёт итоговый комментарий:  
  «Есть признаки использования AI-инструментов, но кандидат демонстрировал понимание при объяснении решения».

---

### 2.7. Boss Fight & Replay (если успеем реализовать)

**Boss Fight**

- В конце интервью:
  - LLM смотрит на слабые места (где фейлы, где много подсказок);
  - генерит **персональную финальную задачу-босса**.
- Кандидат пытается её решить за ограниченное время.
- Результат этой задачи даёт дополнительный коэффициент к финальному грейду.

**Replay**

- Все сабмиты кода сохраняются с timestamp’ами.
- На странице отчёта:
  - есть таймлайн;
  - можно «перематывать», как эволюционировал код.
- LLM может сгенерировать короткий комментарий по ключевым шагам.

---

## 3. Архитектура под стек React + FastAPI + Postgres + Docker

```text
[ React + TypeScript (Frontend) ]
        |
        v
[ FastAPI Backend ]
   |        |            |
   |        |            +--> [ SciBox LLM API ]
   |        |
   |        +--> [ Code Runner (Docker, sandbox) ]
   |
   +------------> [ PostgreSQL ]
```

### 3.1. Backend-структура

```text
backend/
  app/
    main.py                # Точка входа FastAPI
    api/
      interview.py         # /api/interview/*
      admin.py             # /api/admin/*
      resume.py            # /api/resume/*
    models/                # SQLAlchemy/SQLModel сущности
    schemas/               # Pydantic-схемы
    services/
      scibox_client.py     # Обёртка над SciBox LLM
      adaptive.py          # Логика уровней и сложностей
      code_runner.py       # Запуск кода
      anti_cheat.py        # События и trust_score
      reporting.py         # Финальный отчёт, skill radar
    core/
      config.py            # Настройки, env
      db.py                # Подключение к Postgres
```

### 3.2. Frontend-структура

```text
frontend/
  src/
    App.tsx
    pages/
      LandingPage.tsx          # выбор трека, загрузка резюме
      InterviewPage.tsx        # IDE + чат + подсказки
      ResultPage.tsx           # грейд, skill radar, trust_score
      AdminDashboardPage.tsx   # интерфейс компании
    components/
      TaskView.tsx
      IdeEditor.tsx
      ChatPanel.tsx
      HintPanel.tsx
      MetricsPanel.tsx
      GradeProgressBar.tsx
      SkillRadarChart.tsx
      TrustScoreBadge.tsx
    api/                       # клиенты к backend API
```

---

## 4. Настройка и запуск

### 4.1. Переменные окружения

`.env.example`:

```env
# SciBox LLM
SCIBOX_API_KEY=YOUR_SCIBOX_KEY_HERE
SCIBOX_BASE_URL=https://llm.t1v.scibox.tech/v1

# DB
POSTGRES_USER=vibecode
POSTGRES_PASSWORD=vibecode
POSTGRES_DB=vibecode
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

DATABASE_URL=postgresql+psycopg://vibecode:vibecode@postgres:5432/vibecode

# Backend
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000

# CORS
FRONTEND_ORIGIN=http://localhost:5173
```

### 4.2. Docker-запуск

```bash
cd deploy
docker-compose up --build
```

Сервисы:
- Frontend: `http://localhost:5173`
- Backend: `http://localhost:8000`
- Postgres: внутри докера по `postgres:5432`

---

## 5. План по чек-поинтам

### Чек-поинт 1
- Архитектурная схема (как в разделе 3).
- Рабочий интеграционный кусок с SciBox:
  - `scibox_client.py`;
  - `/api/interview/start` → LLM генерит первую задачу.
- Дизайн адаптивности:
  - структура `InterviewState`;
  - правила смены уровня;
  - план CV-based level suggestion и skill radar.

### Чек-поинт 2
- Реализованы:
  - сабмиты задач;
  - базовый code runner;
  - подсказки с ценой (Hint Economy);
  - первичный сбор метрик.
- Демка:
  - старт интервью;
  - смена сложности задач по результатам;
  - использование подсказок и влияние на балл.

### Чек-поинт 3
- Оптимизация (работа с лимитами RPS SciBox, кэш и т.п.).
- Полный сценарий:
  - загрузка резюме → рекомендованный уровень;
  - адаптивное интервью (2–3 задачи);
  - подсказки за цену;
  - AI Bug Hunter;
  - Explanation Check;
  - финальный отчёт с грейд-прогресс-баром, skill radar и trust_score.

---

## 6. Что делать в Cursor Pro

1. Создать репозиторий и положить этот README как `README.md`.
2. На основе разделов 3–4 создать каркас:
   - backend (`main.py`, `api/interview.py`, `services/scibox_client.py`);
   - frontend (`LandingPage`, `InterviewPage`, `ResultPage`).
3. Поэтапно добавлять killer-фичи:
   - сначала CV-based level suggestion + подсказки;
   - затем skill radar;
   - затем Bug Hunter и trust_score.
