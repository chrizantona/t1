"""
SciBox LLM API Client - ASYNC VERSION with KILLER PROMPTS
Wrapper for interacting with SciBox models using OpenAI-compatible API.
"""
from openai import AsyncOpenAI
from typing import List, Dict, Any, Optional
import asyncio
import time
import json
import re

from ..core.config import settings
from .prompts import (
    RESUME_ANALYSIS_SYSTEM, RESUME_ANALYSIS_USER,
    INTERVIEWER_CHAT_SYSTEM, INTERVIEWER_CHAT_USER,
    HINT_SYSTEM, HINT_USER,
    BUG_ANALYSIS_SYSTEM, BUG_ANALYSIS_USER,
    EVALUATE_ANSWER_SYSTEM, EVALUATE_ANSWER_USER,
    COMPLEXITY_QUESTION_SYSTEM, COMPLEXITY_QUESTION_USER,
    AI_DETECTION_SYSTEM, AI_DETECTION_USER,
    FINAL_REPORT_SYSTEM, FINAL_REPORT_USER
)
from ..prompts.task_selection_explainer import (
    TASK_SELECTION_EXPLAINER_SYSTEM, TASK_SELECTION_EXPLAINER_USER,
    TASK_OPENING_QUESTION_SYSTEM, TASK_OPENING_QUESTION_USER,
    SOLUTION_FOLLOWUP_SYSTEM, SOLUTION_FOLLOWUP_USER,
    SOLUTION_ANSWER_EVAL_SYSTEM, SOLUTION_ANSWER_EVAL_USER
)


class SciBoxClient:
    """Client for SciBox LLM API with async rate limiting support."""
    
    def __init__(self):
        self.client = AsyncOpenAI(
            api_key=settings.SCIBOX_API_KEY,
            base_url=settings.SCIBOX_BASE_URL
        )
        self.chat_model = settings.CHAT_MODEL
        self.coder_model = settings.CODER_MODEL
        self.embedding_model = settings.EMBEDDING_MODEL
        
        # Rate limiting with locks
        self._chat_lock = asyncio.Lock()
        self._coder_lock = asyncio.Lock()
        self._embedding_lock = asyncio.Lock()
        
        self._last_chat_request = 0
        self._last_coder_request = 0
        self._last_embedding_request = 0
        self._chat_interval = 1.0 / settings.CHAT_MODEL_RPS
        self._coder_interval = 1.0 / settings.CODER_MODEL_RPS
        self._embedding_interval = 1.0 / settings.EMBEDDING_MODEL_RPS
    
    def _clean_think_tags(self, text: str) -> str:
        """Remove <think> tags and ALL internal reasoning from response."""
        if not text:
            return text
        # Remove <think>...</think> blocks (greedy and non-greedy)
        text = re.sub(r'<think>[\s\S]*?</think>', '', text, flags=re.DOTALL).strip()
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()
        # Remove partial or malformed tags
        text = re.sub(r'</?think[^>]*>', '', text).strip()
        # Remove any remaining <think> without closing
        text = re.sub(r'<think>[\s\S]*$', '', text, flags=re.DOTALL).strip()
        return text
    
    async def _rate_limit(self, last_request_time: float, interval: float) -> None:
        """Async rate limiting - wait if needed without blocking."""
        elapsed = time.time() - last_request_time
        if elapsed < interval:
            await asyncio.sleep(interval - elapsed)
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 512,
        model: Optional[str] = None
    ) -> str:
        """Send chat completion request (async). Auto-cleans <think> tags."""
        async with self._chat_lock:
            await self._rate_limit(self._last_chat_request, self._chat_interval)
            self._last_chat_request = time.time()
            
            try:
                response = await self.client.chat.completions.create(
                    model=model or self.chat_model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                content = response.choices[0].message.content or ""
                # ALWAYS clean think tags from ALL responses
                content = self._clean_think_tags(content)
                return content
            except Exception as e:
                print(f"‚ö†Ô∏è Chat completion error: {e}")
                return ""
    
    async def code_completion(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.2,
        max_tokens: int = 1024
    ) -> str:
        """Send code completion request (async)."""
        async with self._coder_lock:
            await self._rate_limit(self._last_coder_request, self._coder_interval)
            self._last_coder_request = time.time()
            
            try:
                response = await self.client.chat.completions.create(
                    model=self.coder_model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                return response.choices[0].message.content
            except Exception as e:
                print(f"‚ö†Ô∏è Code completion error: {e}")
                return ""
    
    async def get_embedding(self, text: str) -> List[float]:
        """Get embedding for text using bge-m3 model (async)."""
        async with self._embedding_lock:
            await self._rate_limit(self._last_embedding_request, self._embedding_interval)
            self._last_embedding_request = time.time()
            
            try:
                response = await self.client.embeddings.create(
                    model=self.embedding_model,
                    input=text
                )
                return response.data[0].embedding
            except Exception as e:
                print(f"‚ö†Ô∏è Embedding error: {e}")
                return []
    
    def _parse_json_response(self, response: str, fallback: Dict[str, Any]) -> Dict[str, Any]:
        """Helper to parse JSON from LLM response with robust handling."""
        if not response:
            print("‚ö†Ô∏è Empty response from LLM")
            return fallback
            
        try:
            response = response.strip()
            
            # Remove <think> tags if present (qwen3 thinking mode)
            response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()
            
            # Remove markdown code blocks
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                parts = response.split("```")
                for part in parts:
                    part = part.strip()
                    if part.startswith("json"):
                        response = part[4:].strip()
                        break
                    elif part.startswith("{"):
                        response = part
                        break
            
            # Try to find JSON object in response
            if not response.startswith("{"):
                json_match = re.search(r'\{[\s\S]*\}', response)
                if json_match:
                    response = json_match.group()
            
            parsed = json.loads(response)
            print(f"‚úÖ Successfully parsed JSON response")
            return parsed
            
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è JSON parse error: {e}")
            print(f"Raw response (first 500 chars): {response[:500]}")
            return fallback
        except Exception as e:
            print(f"‚ö†Ô∏è Unexpected error parsing response: {e}")
            return fallback
    
    # ========== KILLER PROMPT METHODS ==========
    
    async def analyze_resume(self, resume_text: str) -> Dict[str, Any]:
        """
        üéØ CV Analysis - Senior Tech Recruiter level analysis
        Returns comprehensive candidate assessment with grade, tracks, strengths, weaknesses
        """
        user_prompt = RESUME_ANALYSIS_USER.format(resume_text=resume_text)
        
        messages = [
            {"role": "system", "content": RESUME_ANALYSIS_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.3, max_tokens=1024)
        
        return self._parse_json_response(response, {
            "recommended_grade": "middle",
            "confidence": 50,
            "tracks": ["backend"],
            "years_of_experience": 2,
            "key_technologies": [],
            "strengths": ["–ï—Å—Ç—å –æ–ø—ã—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏"],
            "weaknesses": ["–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"],
            "justification": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏",
            "risk_factors": [],
            "interview_focus": ["–£—Ç–æ—á–Ω–∏—Ç—å –æ–ø—ã—Ç –Ω–∞ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–∏"]
        })
    
    async def chat_with_interviewer(
        self,
        task_text: str,
        level: str,
        direction: str,
        task_title: str,
        user_code: str,
        user_message: str,
        chat_history: List[Dict[str, str]] = None
    ) -> str:
        """
        üí¨ AI Interviewer - Friendly but professional technical interviewer
        Never gives solutions, asks clarifying questions, supports candidate
        """
        user_prompt = INTERVIEWER_CHAT_USER.format(
            level=level,
            direction=direction,
            task_title=task_title,
            task_description=task_text,
            user_code=user_code or "# –ö–∞–Ω–¥–∏–¥–∞—Ç –µ—â–µ –Ω–µ –Ω–∞–ø–∏—Å–∞–ª –∫–æ–¥",
            user_message=user_message
        )
        
        messages = [{"role": "system", "content": INTERVIEWER_CHAT_SYSTEM}]
        
        if chat_history:
            # Add last 10 messages for context
            messages.extend(chat_history[-10:])
        
        messages.append({"role": "user", "content": user_prompt})
        
        response = await self.chat_completion(messages, temperature=0.7, max_tokens=512)
        
        # Return plain text, not JSON
        return response if response else "–•–æ—Ä–æ—à–∏–π –≤–æ–ø—Ä–æ—Å! –î–∞–≤–∞–π —Ä–∞–∑–±–µ—Ä—ë–º—Å—è –≤–º–µ—Å—Ç–µ."
    
    async def generate_hint(
        self,
        task_text: str,
        user_code: str,
        test_results: str,
        hint_level: str
    ) -> Dict[str, Any]:
        """
        üí° Hint Generation - Progressive hints without giving away solution
        Levels: light (-10 pts), medium (-25 pts), heavy (-40 pts)
        """
        user_prompt = HINT_USER.format(
            task_text=task_text,
            user_code=user_code or "# –ö–æ–¥ –ø–æ–∫–∞ –Ω–µ –Ω–∞–ø–∏—Å–∞–Ω",
            test_results=test_results or "–¢–µ—Å—Ç—ã –µ—â–µ –Ω–µ –∑–∞–ø—É—Å–∫–∞–ª–∏—Å—å",
            hint_level=hint_level
        )
        
        messages = [
            {"role": "system", "content": HINT_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.4, max_tokens=512)
        
        return self._parse_json_response(response, {
            "hint_level": hint_level,
            "hint_text": "–ü–æ–ø—Ä–æ–±—É–π –Ω–∞—á–∞—Ç—å —Å —Å–∞–º–æ–≥–æ –ø—Ä–æ—Å—Ç–æ–≥–æ —Å–ª—É—á–∞—è. –ö–∞–∫ –±—ã —Ç—ã —Ä–µ—à–∏–ª –∑–∞–¥–∞—á—É –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –≤—Ö–æ–¥–∞?",
            "encouragement": "–¢—ã –Ω–∞ –≤–µ—Ä–Ω–æ–º –ø—É—Ç–∏, –ø—Ä–æ–¥–æ–ª–∂–∞–π!",
            "next_step": "–û–ø—Ä–µ–¥–µ–ª–∏ –±–∞–∑–æ–≤—ã–π —Å–ª—É—á–∞–π –∏ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É—Å–ª–æ–∂–Ω—è–π"
        })
    
    async def generate_auto_hint_on_failure(
        self,
        task_title: str,
        task_description: str,
        visible_tests: List[Dict],
        user_code: str,
        error_message: str = ""
    ) -> Dict[str, Any]:
        """
        üîÑ Auto-hint on submission failure
        Gives helpful hint about input format, common mistakes, etc.
        Penalty: -15 points from max_score
        """
        # Format visible tests for context
        tests_info = ""
        for i, test in enumerate(visible_tests[:2], 1):
            tests_info += f"–¢–µ—Å—Ç {i}: –≤—Ö–æ–¥={test.get('input')}, –≤—ã—Ö–æ–¥={test.get('expected_output')}\n"
        
        system_prompt = """/no_think
–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–∏. –ö–∞–Ω–¥–∏–¥–∞—Ç –æ—Ç–ø—Ä–∞–≤–∏–ª –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞—Ç—å –ö–†–ê–¢–ö–£–Æ –ø–æ–¥—Å–∫–∞–∑–∫—É (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –∫–æ—Ç–æ—Ä–∞—è –ø–æ–º–æ–∂–µ—Ç –ø–æ–Ω—è—Ç—å:
1. –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —á–∏—Ç–∞—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
2. –ö–∞–∫–æ–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –æ–∂–∏–¥–∞–µ—Ç—Å—è –Ω–∞ –≤—ã—Ö–æ–¥–µ
3. –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏ –≤ –ø–æ–¥–æ–±–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö

–ù–ï –¥–∞–≤–∞–π –≥–æ—Ç–æ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ! –¢–æ–ª—å–∫–æ –Ω–∞–ø—Ä–∞–≤—å –≤ –Ω—É–∂–Ω—É—é —Å—Ç–æ—Ä–æ–Ω—É.

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{
    "hint_text": "–∫—Ä–∞—Ç–∫–∞—è –ø–æ–¥—Å–∫–∞–∑–∫–∞",
    "input_format_tip": "–∫–∞–∫ —á–∏—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ",
    "common_mistake": "—á–∞—Å—Ç–∞—è –æ—à–∏–±–∫–∞"
}"""

        user_prompt = f"""–ó–∞–¥–∞—á–∞: {task_title}
–û–ø–∏—Å–∞–Ω–∏–µ: {task_description}

–ü—Ä–∏–º–µ—Ä—ã —Ç–µ—Å—Ç–æ–≤:
{tests_info}

–ö–æ–¥ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞:
```python
{user_code[:500] if user_code else '# –ø—É—Å—Ç–æ'}
```

{f'–û—à–∏–±–∫–∞: {error_message}' if error_message else '–¢–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ—à–ª–∏'}

–î–∞–π –∫—Ä–∞—Ç–∫—É—é –ø–æ–¥—Å–∫–∞–∑–∫—É."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.4, max_tokens=300)
        
        return self._parse_json_response(response, {
            "hint_text": "–ü—Ä–æ–≤–µ—Ä—å —Ñ–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–∏–ø –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è.",
            "input_format_tip": "–£–±–µ–¥–∏—Å—å, —á—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —á–∏—Ç–∞–µ—à—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.",
            "common_mistake": "–ß–∞—Å—Ç–æ –∑–∞–±—ã–≤–∞—é—Ç –ø—Ä–æ –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏."
        })

    async def analyze_bug(
        self,
        task_description: str,
        user_code: str,
        test_results: str,
        error_message: str = ""
    ) -> Dict[str, Any]:
        """
        üêõ Bug Analysis - Code reviewer explaining why code fails
        Shows failing example, explains bug, hints direction without giving fix
        """
        user_prompt = BUG_ANALYSIS_USER.format(
            task_description=task_description,
            user_code=user_code,
            test_results=test_results,
            error_message=error_message or "–ù–µ—Ç —è–≤–Ω–æ–π –æ—à–∏–±–∫–∏, —Ç–µ—Å—Ç—ã –ø—Ä–æ—Å—Ç–æ –Ω–µ –ø—Ä–æ—Ö–æ–¥—è—Ç"
        )
        
        messages = [
            {"role": "system", "content": BUG_ANALYSIS_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.3, max_tokens=768)
        
        return self._parse_json_response(response, {
            "bug_type": "logic",
            "analysis": "–í –∫–æ–¥–µ –µ—Å—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞. –ü—Ä–æ–≤–µ—Ä—å –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ —É—Å–ª–æ–≤–∏—è –∏ –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏.",
            "failing_example": "–ü–æ–ø—Ä–æ–±—É–π –∑–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–¥ –Ω–∞ –∫—Ä–∞–π–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
            "expected_vs_actual": "–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–∂–∏–¥–∞–µ–º–æ–≥–æ",
            "hint_direction": "–ü–æ–¥—É–º–∞–π, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
            "severity": "major"
        })
    
    async def evaluate_theory_answer(
        self,
        question: str,
        canonical_answer: str,
        key_points: List[str],
        candidate_answer: str,
        difficulty: str = "middle"
    ) -> Dict[str, Any]:
        """
        üìù Theory Answer Evaluation - Expert examiner scoring 0-3
        Checks correctness, completeness, understanding depth
        """
        user_prompt = EVALUATE_ANSWER_USER.format(
            question=question,
            canonical_answer=canonical_answer,
            key_points="\n".join(f"- {p}" for p in key_points) if key_points else "–ù–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö –ø—É–Ω–∫—Ç–æ–≤",
            candidate_answer=candidate_answer,
            difficulty=difficulty
        )
        
        messages = [
            {"role": "system", "content": EVALUATE_ANSWER_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.2, max_tokens=768)
        
        return self._parse_json_response(response, {
            "score": 1,
            "correctness": "–û—Ç–≤–µ—Ç —á–∞—Å—Ç–∏—á–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω",
            "missing": "–¢—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
            "errors": [],
            "feedback_for_candidate": "–û—Ç–≤–µ—Ç –∑–∞—Å—á–∏—Ç–∞–Ω, –Ω–æ –º–æ–∂–Ω–æ –±—ã–ª–æ —Ä–∞—Å–∫—Ä—ã—Ç—å —Ç–µ–º—É –≥–ª—É–±–∂–µ.",
            "extra_topics": [],
            "interviewer_note": "–¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ follow-up"
        })
    
    async def generate_complexity_question(
        self,
        task_title: str,
        task_description: str,
        candidate_code: str
    ) -> Dict[str, Any]:
        """
        ‚è±Ô∏è Complexity Question - Ask about time/space complexity
        Generates natural follow-up question about algorithm complexity
        """
        user_prompt = COMPLEXITY_QUESTION_USER.format(
            task_title=task_title,
            task_description=task_description,
            candidate_code=candidate_code
        )
        
        messages = [
            {"role": "system", "content": COMPLEXITY_QUESTION_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.5, max_tokens=384)
        
        return self._parse_json_response(response, {
            "intro": "–û—Ç–ª–∏—á–Ω–æ, –∑–∞–¥–∞—á–∞ —Ä–µ—à–µ–Ω–∞!",
            "question": "–†–∞—Å—Å–∫–∞–∂–∏, –∫–∞–∫–∞—è –≤—Ä–µ–º–µ–Ω–Ω–∞—è –∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å —É —Ç–≤–æ–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è?",
            "follow_up": "–ê –º–æ–∂–Ω–æ –ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º?"
        })
    
    async def check_ai_likeness(self, user_code: str, level: str = "middle") -> Dict[str, Any]:
        """
        ü§ñ AI Code Detection - Detect AI-generated code patterns
        Returns probability score 0-1 with confidence and signals
        """
        user_prompt = AI_DETECTION_USER.format(
            code=user_code,
            level=level
        )
        
        messages = [
            {"role": "system", "content": AI_DETECTION_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.code_completion(messages, temperature=0.2, max_tokens=512)
        
        return self._parse_json_response(response, {
            "ai_style_score": 0.3,
            "confidence": "low",
            "signals": [],
            "human_signals": ["–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∏–ª—å –∫–æ–¥–∞"],
            "verdict": "likely_human",
            "recommendation": "–°–ø—Ä–æ—Å–∏—Ç—å –ø—Ä–æ –¥–µ—Ç–∞–ª–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏"
        })
    
    async def generate_final_report(self, raw_metrics_json: str) -> Dict[str, Any]:
        """
        üìä Final Report Generation - Professional interview summary
        Decision: hire/consider/reject with skills breakdown and recommendations
        """
        user_prompt = FINAL_REPORT_USER.format(interview_data=raw_metrics_json)
        
        messages = [
            {"role": "system", "content": FINAL_REPORT_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.3, max_tokens=1500)
        
        return self._parse_json_response(response, {
            "overall_grade": "middle",
            "overall_score": 70,
            "decision": "consider",
            "decision_reasoning": "–ö–∞–Ω–¥–∏–¥–∞—Ç –ø–æ–∫–∞–∑–∞–ª —Å—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ.",
            "skills": {
                "algorithms": {"score": 70, "comment": "–ë–∞–∑–æ–≤—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∑–Ω–∞–µ—Ç"},
                "architecture": {"score": 65, "comment": "–°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å"},
                "clean_code": {"score": 75, "comment": "–ö–æ–¥ —á–∏—Ç–∞–µ–º—ã–π"},
                "debugging": {"score": 60, "comment": "–ï—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª"},
                "communication": {"score": 70, "comment": "–û–±—ä—è—Å–Ω—è–µ—Ç –ø–æ–Ω—è—Ç–Ω–æ"}
            },
            "strengths": ["–ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"],
            "areas_to_improve": ["–ê–ª–≥–æ—Ä–∏—Ç–º—ã", "–°–∏—Å—Ç–µ–º–Ω—ã–π –¥–∏–∑–∞–π–Ω"],
            "candidate_feedback": "–•–æ—Ä–æ—à–∞—è —Ä–∞–±–æ—Ç–∞! –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø–æ–¥—Ç—è–Ω—É—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö.",
            "hiring_manager_notes": "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞",
            "next_steps": ["–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∏–Ω—Ç–µ—Ä–≤—å—é —Å –∫–æ–º–∞–Ω–¥–æ–π"]
        })
    
    async def generate_task_selection_reason(
        self,
        task_payload: Dict[str, Any],
        track: str,
        difficulty: str,
        target_skills: List[str],
        candidate_level: str,
        direction: str,
        block_type: str = "algo",
        vacancy_info: str = "",
        candidate_additional_info: str = ""
    ) -> str:
        """
        üéØ Task Selection Explainer - Explain WHY this task was selected
        Returns human-readable explanation (3-5 sentences)
        """
        import json as json_module
        
        user_prompt = TASK_SELECTION_EXPLAINER_USER.format(
            vacancy_info=vacancy_info or "–ü—Ä—è–º–æ–µ –∏–Ω—Ç–µ—Ä–≤—å—é –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –≤–∞–∫–∞–Ω—Å–∏–∏",
            candidate_level=candidate_level,
            direction=direction,
            candidate_additional_info=candidate_additional_info or "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
            block_type=block_type,
            track=track,
            difficulty=difficulty,
            target_skills=json_module.dumps(target_skills, ensure_ascii=False),
            task_payload=json_module.dumps(task_payload, ensure_ascii=False, indent=2)
        )
        
        messages = [
            {"role": "system", "content": TASK_SELECTION_EXPLAINER_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.3, max_tokens=512)
        
        result = self._parse_json_response(response, {
            "selection_reason": f"–ó–∞–¥–∞—á–∞ –ø–æ–¥–æ–±—Ä–∞–Ω–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞–≤—ã–∫–æ–≤ {', '.join(target_skills)} –Ω–∞ —É—Ä–æ–≤–Ω–µ {difficulty} –¥–ª—è {direction}-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ —É—Ä–æ–≤–Ω—è {candidate_level}."
        })
        
        return result.get("selection_reason", "–ó–∞–¥–∞—á–∞ –ø–æ–¥–æ–±—Ä–∞–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥ –≤–∞—à –ø—Ä–æ—Ñ–∏–ª—å.")
    
    async def generate_opening_question(
        self,
        task_description: str,
        block_type: str,
        track: str,
        candidate_grade: str,
        difficulty: str
    ) -> str:
        """
        üí¨ Opening Question - Generate first smart question for the task
        Returns single question to start dialogue with candidate
        """
        user_prompt = TASK_OPENING_QUESTION_USER.format(
            task_description=task_description,
            block_type=block_type,
            track=track,
            candidate_grade=candidate_grade,
            difficulty=difficulty
        )
        
        messages = [
            {"role": "system", "content": TASK_OPENING_QUESTION_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.7, max_tokens=256)
        
        # Clean response
        if response:
            response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()
            response = response.strip('"').strip("'")
        
        if not response:
            # Fallback questions based on track
            fallback_questions = {
                "backend": "ü§î –ö–∞–∫—É—é –∞—Å–∏–º–ø—Ç–æ—Ç–∏—á–µ—Å–∫—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç—ã –ø–ª–∞–Ω–∏—Ä—É–µ—à—å –¥–æ—Å—Ç–∏—á—å –≤ —Å–≤–æ—ë–º —Ä–µ—à–µ–Ω–∏–∏?",
                "algorithms": "ü§î –ö–∞–∫—É—é –∞—Å–∏–º–ø—Ç–æ—Ç–∏—á–µ—Å–∫—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç—ã –ø–ª–∞–Ω–∏—Ä—É–µ—à—å –¥–æ—Å—Ç–∏—á—å –≤ —Å–≤–æ—ë–º —Ä–µ—à–µ–Ω–∏–∏?",
                "ml": "üìä –° —á–µ–≥–æ –±—ã —Ç—ã –Ω–∞—á–∞–ª —Ä–∞–±–æ—Ç—É —Å –¥–∞–Ω–Ω—ã–º–∏ –≤ —ç—Ç–æ–π –∑–∞–¥–∞—á–µ?",
                "data-science": "üìä –ö–∞–∫–∏–µ —à–∞–≥–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Ç—ã –±—ã —Å–¥–µ–ª–∞–ª –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å?",
                "frontend": "üé® –ö–∞–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Ç–µ–±–µ –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏?",
                "devops": "üîß –ö–∞–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ –ø–æ–¥—Ö–æ–¥—ã —Ç—ã –±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª?",
                "data": "üìä –ö–∞–∫ –±—ã —Ç—ã –ø—Ä–æ–≤–µ—Ä–∏–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤?"
            }
            response = fallback_questions.get(track, "ü§î –ö–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ —Ç—ã –ø–ª–∞–Ω–∏—Ä—É–µ—à—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏?")
        
        return response
    
    async def generate_solution_followup_question(
        self,
        task_title: str,
        task_description: str,
        candidate_code: str,
        candidate_level: str,
        difficulty: str
    ) -> str:
        """
        üéØ Solution Follow-up Question - Ask about the solution after task completion
        Returns single question about complexity, algorithm, optimization etc.
        """
        user_prompt = SOLUTION_FOLLOWUP_USER.format(
            task_title=task_title,
            task_description=task_description,
            candidate_code=candidate_code,
            candidate_level=candidate_level,
            difficulty=difficulty
        )
        
        messages = [
            {"role": "system", "content": SOLUTION_FOLLOWUP_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.6, max_tokens=256)
        
        # Clean response - remove any <think> tags
        response = self._clean_think_tags(response)
        
        if not response:
            # Fallback questions
            fallback = [
                f"ü§î –ö–∞–∫–æ–≤–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–≤–æ–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è? –ê –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è?",
                f"ü§î –ú–æ–∂–µ—à—å –æ–±—ä—è—Å–Ω–∏—Ç—å, –ø–æ—á–µ–º—É —Ç—ã –≤—ã–±—Ä–∞–ª –∏–º–µ–Ω–Ω–æ —Ç–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥?",
                f"ü§î –ú–æ–∂–Ω–æ –ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ —Ä–µ—à–µ–Ω–∏–µ? –ö–∞–∫?",
            ]
            import random
            response = random.choice(fallback)
        
        return response
    
    async def evaluate_solution_answer(
        self,
        task_title: str,
        candidate_code: str,
        question: str,
        candidate_answer: str,
        candidate_level: str
    ) -> Dict[str, Any]:
        """
        üìù Evaluate Solution Answer - Score candidate's answer about their solution
        Returns score (0-100) with feedback
        """
        user_prompt = SOLUTION_ANSWER_EVAL_USER.format(
            task_title=task_title,
            candidate_code=candidate_code,
            question=question,
            candidate_answer=candidate_answer,
            candidate_level=candidate_level
        )
        
        messages = [
            {"role": "system", "content": SOLUTION_ANSWER_EVAL_SYSTEM},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.2, max_tokens=512)
        
        # Clean response
        response = self._clean_think_tags(response)
        
        return self._parse_json_response(response, {
            "score": 50,
            "correctness": 50,
            "completeness": 50,
            "understanding": 50,
            "feedback": "–û—Ç–≤–µ—Ç —É—á—Ç—ë–Ω –≤ –æ—Ü–µ–Ω–∫–µ.",
            "correct_answer": None
        })
    
    # ========== LEGACY METHODS (for backward compatibility) ==========
    
    async def generate_task(
        self,
        level: str,
        track: str,
        history_summary: str = ""
    ) -> Dict[str, Any]:
        """Generate adaptive task (legacy - prefer task_pool)"""
        system_prompt = """/no_think
–¢—ã ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –û–î–ù–£ –∑–∞–¥–∞—á—É –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.
–£—Ä–æ–≤–µ–Ω—å: {level}. –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {track}.

–§–æ—Ä–º–∞—Ç JSON:
{{
  "title": "–Ω–∞–∑–≤–∞–Ω–∏–µ",
  "description": "—É—Å–ª–æ–≤–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º",
  "input_format": "—Ñ–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–∞",
  "output_format": "—Ñ–æ—Ä–º–∞—Ç –≤—ã—Ö–æ–¥–∞",
  "examples": [{{"input": "...", "output": "...", "explanation": "..."}}],
  "constraints": "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è",
  "difficulty_level": "{level}",
  "topic_tags": ["..."]
}}""".format(level=level, track=track)
        
        user_prompt = f"""–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∑–∞–¥–∞—á—É –¥–ª—è {level} {track}-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞.
–ò—Å—Ç–æ—Ä–∏—è: {history_summary or '–ü–µ—Ä–≤–∞—è –∑–∞–¥–∞—á–∞'}"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.4, max_tokens=1024)
        
        return self._parse_json_response(response, {
            "title": "–°—É–º–º–∞ –¥–≤—É—Ö —á–∏—Å–µ–ª",
            "description": "–ù–∞–ø–∏—à–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—É–º–º—É –¥–≤—É—Ö —á–∏—Å–µ–ª.",
            "examples": [{"input": "2, 3", "output": "5", "explanation": "2+3=5"}],
            "difficulty_level": level,
            "topic_tags": ["math"]
        })
    
    async def generate_bug_hunter_tests(
        self,
        task_text: str,
        user_code: str,
        known_tests: str
    ) -> Dict[str, Any]:
        """Generate edge case tests to break candidate's code"""
        system_prompt = """/no_think
–¢—ã ‚Äî Bug Hunter. –ù–∞–π–¥–∏ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –≤ –∫–æ–¥–µ –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Ç–µ—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ–≥–æ —Å–ª–æ–º–∞—é—Ç.

JSON –æ—Ç–≤–µ—Ç:
{
  "generated_tests": [
    {"input": "—Ç–µ—Å—Ç", "description": "–ø–æ—á–µ–º—É —Å–ª–æ–º–∞–µ—Ç"}
  ]
}"""
        
        user_prompt = f"""–ó–∞–¥–∞—á–∞: {task_text}
–ö–æ–¥: {user_code}
–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã: {known_tests}

–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 3-5 —Ç–µ—Å—Ç–æ–≤-edge cases."""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.code_completion(messages, temperature=0.3, max_tokens=512)
        
        return self._parse_json_response(response, {"generated_tests": []})
    
    async def generate_edge_case_tests_enhanced(
        self,
        task_description: str,
        input_format: str,
        output_format: str,
        examples: str,
        candidate_code: str,
        existing_tests: str
    ) -> Dict[str, Any]:
        """Enhanced Bug Hunter with security checks"""
        from .code_sanitizer import sanitize_code_for_llm, get_security_summary
        
        security_report = get_security_summary(candidate_code)
        
        if security_report["prompt_injection"]["detected"]:
            return {
                "security_blocked": True,
                "security_report": security_report,
                "analysis": {
                    "detected_algorithm": "BLOCKED",
                    "potential_weaknesses": ["–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø–æ–ø—ã—Ç–∫–∞ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏"],
                    "missing_checks": []
                },
                "edge_case_tests": []
            }
        
        sanitized_code, sanitize_report = sanitize_code_for_llm(candidate_code)
        
        result = await self.generate_bug_hunter_tests(
            task_description, 
            sanitized_code, 
            existing_tests
        )
        
        result["security_report"] = security_report
        result["sanitize_report"] = sanitize_report
        result["security_blocked"] = False
        
        return result
    
    async def check_explanation(
        self,
        task_text: str,
        user_code: str,
        user_explanation: str
    ) -> Dict[str, Any]:
        """Check if candidate understands their solution"""
        system_prompt = """/no_think
–û—Ü–µ–Ω–∏, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–¥–∏–¥–∞—Ç –ø–æ–Ω–∏–º–∞–µ—Ç —Å–≤–æ—ë —Ä–µ—à–µ–Ω–∏–µ.

JSON:
{
  "communication_score": 0-100,
  "understanding_level": "low|medium|high",
  "comment": "–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"
}"""
        
        user_prompt = f"""–ó–∞–¥–∞—á–∞: {task_text}
–ö–æ–¥: {user_code}
–û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {user_explanation}"""
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = await self.chat_completion(messages, temperature=0.3, max_tokens=384)
        
        return self._parse_json_response(response, {
            "communication_score": 70,
            "understanding_level": "medium",
            "comment": "–ö–∞–Ω–¥–∏–¥–∞—Ç –ø–æ–Ω–∏–º–∞–µ—Ç –æ—Å–Ω–æ–≤—ã —Å–≤–æ–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è"
        })
    
    async def generate_boss_fight_task(self, interview_weaknesses_json: str) -> Dict[str, Any]:
        """Generate personalized final challenge based on weaknesses"""
        return await self.generate_task("senior", "algorithms", interview_weaknesses_json)


# Global client instance
scibox_client = SciBoxClient()

# –ø–∏–¥–æ—Ä–º–æ—Ç
